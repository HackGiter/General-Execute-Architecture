[[32mINFO[0m|gea.model.template:75] 09/18/2024 03:19:43 >> Model initialize
[[32mINFO[0m|__main__:33] 09/18/2024 03:19:58 >> Pretrained embed tokens loaded
[[32mINFO[0m|__main__:37] 09/18/2024 03:20:01 >> Pretrained lm head loaded
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> model.embed_tokens.weight
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> model.decoder.self_attn.q_proj.weight
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> model.decoder.self_attn.kv_proj.weight
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> model.decoder.self_attn.o_proj.weight
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> model.decoder.mlp.gate_proj.weight
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> model.decoder.mlp.up_proj.weight
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> model.decoder.mlp.down_proj.weight
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> model.decoder.prev_attn_layernorm.weight
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> model.decoder.post_attn_ln_v1.weight
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> model.decoder.post_attn_ln_v2.weight
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> model.norm_v1.weight
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> model.norm_v2.weight
[[34mDEBUG[0m|__main__:47] 09/18/2024 03:20:01 >> lm_head.weight
[[32mINFO[0m|gea.model.template:95] 09/18/2024 03:20:01 >> 
xAdaptiveForCausalLM(
  (model): xAdaptiveModel(
    (embed_tokens): Embedding(32000, 4096) ( weight:False )
    (decoder): xAdaptiveDecoderLayer(
      (self_attn): xAdaptiveAttention(
        (q_proj): Linear(in_features=4096, out_features=20480, bias=False) ( weight:True )
        (kv_proj): Linear(in_features=8192, out_features=8192, bias=False) ( weight:True )
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False) ( weight:True )
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): xAdaptiveMLP(
        (gate_proj): Linear(in_features=8192, out_features=11008, bias=False) ( weight:True )
        (up_proj): Linear(in_features=8192, out_features=11008, bias=False) ( weight:True )
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False) ( weight:True )
        (act_fn): SiLU()
      )
      (prev_attn_layernorm): LlamaRMSNorm() ( weight:True )
      (post_attn_ln_v1): LlamaRMSNorm() ( weight:True )
      (post_attn_ln_v2): LlamaRMSNorm() ( weight:True )
    )
    (norm_v1): LlamaRMSNorm() ( weight:True )
    (norm_v2): LlamaRMSNorm() ( weight:True )
  )
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False) ( weight:False )
)
[[32mINFO[0m|gea.train.train:64] 09/18/2024 03:20:03 >> Tensoboard tracker initialize
[[32mINFO[0m|gea.utils.callback:289] 09/18/2024 03:20:04 >> Trainer initialize
[[32mINFO[0m|gea.train.train:203] 09/18/2024 03:20:04 >> Optimizer initialize: ADAMW
[[34mDEBUG[0m|__main__:135] 09/18/2024 03:20:04 >> {'lr': 0.00024, 'weight_decay': 0.1, 'betas': (0.9, 0.999), 'eps': 1e-08, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None}
[[32mINFO[0m|gea.train.train:219] 09/18/2024 03:20:04 >> LR scheduler initialize: COSINE_WITH_MIN_LR
[[32mINFO[0m|gea.train.train:257] 09/18/2024 03:20:07 >>  ***** Running training *****  
[[32mINFO[0m|gea.train.train:258] 09/18/2024 03:20:07 >>  Num examples = 643,200
[[32mINFO[0m|gea.train.train:259] 09/18/2024 03:20:07 >>  Num Epochs = 10
[[32mINFO[0m|gea.train.train:260] 09/18/2024 03:20:07 >>  Instantaneous batch size per device = 1
[[32mINFO[0m|gea.train.train:262] 09/18/2024 03:20:07 >>  Training with DataParallel so batch size has been adjusted to: 3
[[32mINFO[0m|gea.train.train:263] 09/18/2024 03:20:07 >>  Total train batch size (w. parallel, distributed & accumulation) = 48
[[32mINFO[0m|gea.train.train:264] 09/18/2024 03:20:07 >>  Gradient Accumulation steps = 16
[[32mINFO[0m|gea.train.train:265] 09/18/2024 03:20:07 >>  Total optimization steps = 13,400
[[32mINFO[0m|gea.train.train:266] 09/18/2024 03:20:07 >>  Number of trainable parameters = 393,236,480
[[32mINFO[0m|gea.utils.integration:74] 09/18/2024 03:20:07 >> Tensorboard tracker initialize
  0%|          | 0/13400 [00:00<?, ?it/s]  0%|          | 2/13400 [00:17<32:05:31,  8.62s/it][rank2]: Traceback (most recent call last):
[rank2]:   File "/home/lhz/miniconda3/envs/workplace/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank2]:     return _run_code(code, main_globals, None,
[rank2]:   File "/home/lhz/miniconda3/envs/workplace/lib/python3.10/runpy.py", line 86, in _run_code
[rank2]:     exec(code, run_globals)
[rank2]:   File "/home/lhz/Workplace/General Execute Architecture/gea/self/xAdaptive/v3/xAdaptive_executor.py", line 315, in <module>
[rank2]:     main()
[rank2]:   File "/home/lhz/Workplace/General Execute Architecture/gea/self/xAdaptive/v3/xAdaptive_executor.py", line 312, in main
[rank2]:     run_train(kwargs)
[rank2]:   File "/home/lhz/Workplace/General Execute Architecture/gea/execute/executor.py", line 39, in run_train
[rank2]:     trainer.train()
[rank2]:   File "/home/lhz/Workplace/General Execute Architecture/gea/train/train.py", line 275, in train
[rank2]:     execute_train_process = self.execute_train_process(
[rank2]:   File "/home/lhz/Workplace/General Execute Architecture/gea/train/train.py", line 321, in execute_train_process
[rank2]:     self.accelerator.backward(loss)
[rank2]:   File "/home/lhz/miniconda3/envs/workplace/lib/python3.10/site-packages/accelerate/accelerator.py", line 2196, in backward
[rank2]:     loss.backward(**kwargs)
[rank2]:   File "/home/lhz/miniconda3/envs/workplace/lib/python3.10/site-packages/torch/_tensor.py", line 525, in backward
[rank2]:     torch.autograd.backward(
[rank2]:   File "/home/lhz/miniconda3/envs/workplace/lib/python3.10/site-packages/torch/autograd/__init__.py", line 267, in backward
[rank2]:     _engine_run_backward(
[rank2]:   File "/home/lhz/miniconda3/envs/workplace/lib/python3.10/site-packages/torch/autograd/graph.py", line 744, in _engine_run_backward
[rank2]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank2]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU  has a total capacity of 23.64 GiB of which 733.69 MiB is free. Including non-PyTorch memory, this process has 22.92 GiB memory in use. Of the allocated memory 15.80 GiB is allocated by PyTorch, and 6.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0918 03:20:26.941000 140538663585600 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 769601 closing signal SIGTERM
W0918 03:20:26.943000 140538663585600 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 769602 closing signal SIGTERM
E0918 03:20:28.111000 140538663585600 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 2 (pid: 769603) of binary: /home/lhz/miniconda3/envs/workplace/bin/python
Traceback (most recent call last):
  File "/home/lhz/miniconda3/envs/workplace/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/home/lhz/miniconda3/envs/workplace/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/home/lhz/miniconda3/envs/workplace/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1165, in launch_command
    multi_gpu_launcher(args)
  File "/home/lhz/miniconda3/envs/workplace/lib/python3.10/site-packages/accelerate/commands/launch.py", line 799, in multi_gpu_launcher
    distrib_run.run(args)
  File "/home/lhz/miniconda3/envs/workplace/lib/python3.10/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/home/lhz/miniconda3/envs/workplace/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/lhz/miniconda3/envs/workplace/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
gea.self.xAdaptive.v3.xAdaptive_executor FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-18_03:20:26
  host      : zju
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 769603)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
